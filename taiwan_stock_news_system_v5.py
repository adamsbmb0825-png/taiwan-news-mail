# -*- coding: utf-8 -*-
"""
å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  v5.3 (GitHubæ­£æœ¬)
- æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ ªä¾¡ãƒ•ã‚§ãƒ¼ã‚ºåˆ†æï¼‰ã®è¿½åŠ 
- SendGridé€ä¿¡å…ƒã‚¢ãƒ‰ãƒ¬ã‚¹ã®ä¿®æ­£
- ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹å¼·åˆ¶æ¡ç”¨ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ0ä»¶é˜²æ­¢ï¼‰ã®è¿½åŠ 
"""

import os
import sys
import json
import time
import feedparser
import requests
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from openai import OpenAI
from email_template_v5 import create_email_body, send_email_via_sendgrid
from news_clustering import cluster_news_by_topic, prepare_delivery_news, print_clustering_log
from delayed_valuable_news import is_delayed_but_valuable
from stock_price_analyzer import get_stock_price_data
from investment_aux_generator import generate_investment_aux_news

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
VERSION = "v5.3-20260122-forced-pick"

# ç’°å¢ƒå¤‰æ•°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SENDGRID_API_KEY = os.getenv("SENDGRID_API_KEY")
RECIPIENT_EMAIL = os.getenv("RECIPIENT_EMAIL")

# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®š
JST = timezone(timedelta(hours=9))

# éŠ˜æŸ„ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
try:
    with open("stocks.json", "r", encoding="utf-8") as f:
        json_data = json.load(f)
        STOCKS = json_data.get("stocks", {})
except FileNotFoundError:
    print("âŒ stocks.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    sys.exit(1)

# RSSãƒ•ã‚£ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
RSS_FEEDS = [
    "https://news.google.com/rss/search?q=å°ç©é›»+when:7d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å‰µè¦‹+when:7d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å®‡ç»+when:7d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å»£é”+when:7d&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://money.udn.com/rssfeed/news/1001/5590", # ç”£æ¥­
    "https://money.udn.com/rssfeed/news/1001/5591", # è¨¼åˆ¸
]

client = OpenAI(api_key=OPENAI_API_KEY)

def fetch_rss_feeds(days_back=7):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    print(f"ğŸ“° RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­... (éå»{days_back}æ—¥åˆ†)", flush=True)
    all_entries = []
    seen_links = set()
    
    cutoff_date = datetime.now(JST) - timedelta(days=days_back)
    
    for feed_url in RSS_FEEDS:
        # 30æ—¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯Google Newsã®ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´
        if days_back > 7 and "when:7d" in feed_url:
            feed_url = feed_url.replace("when:7d", "when:30d")
            
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            # æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹
            published = None
            if hasattr(entry, 'published_parsed'):
                published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc).astimezone(JST)
            
            if published and published > cutoff_date:
                if entry.link not in seen_links:
                    all_entries.append({
                        'title': entry.title,
                        'link': entry.link,
                        'summary': getattr(entry, 'summary', ''),
                        'published': published,
                        'source': getattr(entry, 'source', {}).get('title', 'Unknown')
                    })
                    seen_links.add(entry.link)
                    
    print(f"  RSSåé›†å®Œäº†: {len(all_entries)}ä»¶", flush=True)
    return all_entries

def resolve_redirects(entries):
    """Google Newsãªã©ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‚’è§£æ±ºï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰"""
    print(f"ğŸ”— URLè§£æ±ºä¸­ï¼ˆ{len(entries)}ä»¶ï¼‰...", flush=True)
    
    # ä»¶æ•°ãŒå¤šã„å ´åˆã¯æœ€æ–°ã®ã‚‚ã®ã«çµã‚‹ï¼ˆAPIåˆ¶é™å›é¿ï¼‰
    if len(entries) > 100:
        print("  âš ï¸ ä»¶æ•°ãŒå¤šã„ãŸã‚ã€æœ€æ–°100ä»¶ã®ã¿å‡¦ç†ã—ã¾ã™", flush=True)
        entries = sorted(entries, key=lambda x: x['published'], reverse=True)[:100]
        
    def resolve_url(entry):
        try:
            # Google Newsã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè§£æ±º
            if "news.google.com" in entry['link']:
                response = requests.get(entry['link'], timeout=5, allow_redirects=True)
                entry['link'] = response.url
        except:
            pass
        return entry

    with ThreadPoolExecutor(max_workers=10) as executor:
        list(executor.map(resolve_url, entries))
        
    # é‡è¤‡æ’é™¤ï¼ˆURLè§£æ±ºå¾Œï¼‰
    unique_entries = []
    seen_urls = set()
    for entry in entries:
        if entry['link'] not in seen_urls:
            unique_entries.append(entry)
            seen_urls.add(entry['link'])
            
    print(f"âœ… é‡è¤‡é™¤å¤–å¾Œ: {len(unique_entries)}ä»¶", flush=True)
    return unique_entries

def analyze_relevance_with_llm(entry, stock_code, stock_info, is_fallback_mode=False):
    """LLMã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é–¢é€£æ€§ã¨é‡è¦åº¦ã‚’åˆ¤å®š"""
    prompt = f"""
    ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãŒã€å°æ¹¾ã®éŠ˜æŸ„ã€Œ{stock_info['name']} ({stock_code})ã€ã®æ ªä¾¡ã‚„æ¥­ç¸¾ã«ç›´æ¥å½±éŸ¿ã‚’ä¸ãˆã‚‹é‡è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã©ã†ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
    
    ã‚¿ã‚¤ãƒˆãƒ«: {entry['title']}
    è¦ç´„: {entry['summary']}
    
    ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
    {{
        "is_relevant": true/false,
        "reason": "åˆ¤å®šç†ç”±ï¼ˆæ—¥æœ¬èªï¼‰",
        "summary": "æŠ•è³‡å®¶å‘ã‘ã®ç°¡æ½”ãªè¦ç´„ï¼ˆæ—¥æœ¬èªã€50æ–‡å­—ä»¥å†…ï¼‰",
        "importance": 1-5ã®æ•´æ•°ï¼ˆ5ãŒæœ€é«˜ï¼‰
    }}
    
    åˆ¤å®šåŸºæº–:
    - éŠ˜æŸ„åãŒæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹ã€ã¾ãŸã¯ä¸»è¦è£½å“ãƒ»ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ã«æ·±ãé–¢ã‚ã‚‹å ´åˆã¯True
    - å˜ãªã‚‹å¸‚æ³æ¦‚æ³ã‚„ã€åå‰ãŒç¾…åˆ—ã•ã‚Œã¦ã„ã‚‹ã ã‘ã®è¨˜äº‹ã¯False
    - {stock_info.get('keywords', [])} ã«é–¢é€£ã™ã‚‹å…·ä½“çš„ãªå‹•å‘ãŒã‚ã‚Œã°True
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å°æ¹¾æ ªã®å°‚é–€ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚å³æ ¼ã«æƒ…å ±ã®ä¾¡å€¤ã‚’åˆ¤å®šã—ã¾ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.0
        )
        
        result = json.loads(response.choices[0].message.content)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã‹ã¤ã€Œé–¢é€£ãªã—ã€ã®å ´åˆã€é…å»¶ä¾¡å€¤åˆ¤å®šã‚’è©¦è¡Œ
        if is_fallback_mode and not result['is_relevant']:
            is_valuable, reason = is_delayed_but_valuable(entry, stock_info)
            if is_valuable:
                result['is_relevant'] = True
                result['reason'] = f"ã€é…å»¶ä¾¡å€¤ã‚ã‚Šã€‘{reason}"
                
        return result
        
    except Exception as e:
        return {"is_relevant": False, "reason": f"ã‚¨ãƒ©ãƒ¼: {e}", "summary": ""}

def force_pick_news(candidates, stock_info):
    """
    ã€å¼·åˆ¶æ¡ç”¨ãƒ­ã‚¸ãƒƒã‚¯ã€‘
    LLMåˆ¤å®šã§0ä»¶ã«ãªã£ãŸå ´åˆã€å€™è£œã®ä¸­ã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’1ã¤å¼·åˆ¶çš„ã«é¸ã¶ã€‚
    å„ªå…ˆé †ä½:
    1. é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç‡Ÿæ”¶, æ³•èªªæœƒ, å±•æœ›ãªã©ï¼‰ã‚’å«ã‚€è¨˜äº‹
    2. ã‚¿ã‚¤ãƒˆãƒ«ã«éŠ˜æŸ„åãŒå«ã¾ã‚Œã‚‹è¨˜äº‹
    3. æœ€ã‚‚æ–°ã—ã„è¨˜äº‹
    """
    if not candidates:
        return None

    # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    priority_keywords = ["ç‡Ÿæ”¶", "æ³•èªªæœƒ", "è²¡æ¸¬", "å±•æœ›", "æ¥å–®", "CapEx", "DRAM", "NAND", "HBM", "CoWoS", "é—œç¨…", "ç®¡åˆ¶", "EPS", "ç²åˆ©"]
    
    # 1. é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ã‚‚ã®ã‚’æ¢ã™
    for entry in candidates:
        text = (entry['title'] + entry['summary']).lower()
        for kw in priority_keywords:
            if kw.lower() in text:
                print(f"  âš ï¸ FORCED PICK used: {stock_info['name']} reason=Keyword match ({kw}) url={entry['link']}", flush=True)
                return {
                    **entry,
                'llm_result': {
                    'is_relevant': True,
                    'reason': f"ã€è‡ªå‹•è£œå®Œã€‘é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{kw}ã€ã‚’å«ã‚€ãŸã‚å¼·åˆ¶æ¡ç”¨",
                    'summary': f"ã€è‡ªå‹•è£œå®Œã€‘{entry['title']}ï¼ˆ{kw}é–¢é€£ï¼‰",
                    'importance': 3,
                    'representative_reason': f"é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{kw}ã€ã‚’å«ã‚€ãŸã‚"
                },
                'forced_pick': True
            }

    # 2. ã‚¿ã‚¤ãƒˆãƒ«ã«éŠ˜æŸ„åãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’æ¢ã™
    for entry in candidates:
        if stock_info['name'] in entry['title']:
            print(f"  âš ï¸ FORCED PICK used: {stock_info['name']} reason=Title match url={entry['link']}", flush=True)
            return {
                **entry,
                'llm_result': {
                    'is_relevant': True,
                    'reason': "ã€è‡ªå‹•è£œå®Œã€‘ã‚¿ã‚¤ãƒˆãƒ«ã«éŠ˜æŸ„åã‚’å«ã‚€ãŸã‚å¼·åˆ¶æ¡ç”¨",
                    'summary': f"ã€è‡ªå‹•è£œå®Œã€‘{entry['title']}",
                    'importance': 3,
                    'representative_reason': "ã‚¿ã‚¤ãƒˆãƒ«ã«éŠ˜æŸ„åã‚’å«ã‚€ãŸã‚"
                },
                'forced_pick': True
            }

    # 3. ãªã‘ã‚Œã°æœ€æ–°ã®ã‚‚ã®ã‚’æ¡ç”¨
    entry = candidates[0] # candidatesã¯æ—¢ã«æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å‰æ
    print(f"  âš ï¸ FORCED PICK used: {stock_info['name']} reason=Newest fallback url={entry['link']}", flush=True)
    return {
        **entry,
        'llm_result': {
            'is_relevant': True,
            'reason': "ã€è‡ªå‹•è£œå®Œã€‘é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸è¶³ã®ãŸã‚æœ€æ–°è¨˜äº‹ã‚’æ¡ç”¨",
            'summary': f"ã€è‡ªå‹•è£œå®Œã€‘{entry['title']}",
            'importance': 1,
            'representative_reason': "æœ€æ–°è¨˜äº‹ã®ãŸã‚"
        },
        'forced_pick': True
    }

def process_stock_news(stock_code, stock_info, all_entries, is_fallback_mode=False):
    """ç‰¹å®šã®éŠ˜æŸ„ã«é–¢ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å‡¦ç†"""
    print(f"============================================================", flush=True)
    print(f"ğŸ“Š {stock_info['name']}ï¼ˆ{stock_code}ï¼‰", flush=True)
    print(f"============================================================", flush=True)
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ1æ¬¡é¸åˆ¥ï¼‰
    keywords = [stock_info['name'], stock_code]
    if 'keywords' in stock_info:
        keywords.extend(stock_info['keywords'])
        
    candidates = []
    for entry in all_entries:
        text = (entry['title'] + ' ' + entry['summary']).lower()
        if any(k.lower() in text for k in keywords):
            candidates.append(entry)
            
    print(f"å€™è£œãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(candidates)}ä»¶", flush=True)
    
    # å€™è£œãŒå¤šã™ãã‚‹å ´åˆã¯çµã‚Šè¾¼ã¿ï¼ˆLLMã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ä¸Šé™ã‚’å³ã—ãã™ã‚‹ï¼ˆ10ä»¶ï¼‰ã€é€šå¸¸æ™‚ã¯60ä»¶
    limit = 10 if is_fallback_mode else 60
    if len(candidates) > limit:
        candidates = sorted(candidates, key=lambda x: x['published'] or datetime.min, reverse=True)[:limit]
    
    # 2. LLMã«ã‚ˆã‚‹é–¢é€£æ€§åˆ¤å®š
    relevant_news = []
    news_summaries_for_aux = [] # æŠ•è³‡åˆ¤æ–­è£œåŠ©ç”¨ã®è¦ç´„ãƒªã‚¹ãƒˆ
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_entry = {
            executor.submit(analyze_relevance_with_llm, entry, stock_code, stock_info, is_fallback_mode): entry 
            for entry in candidates
        }
        
        for future in as_completed(future_to_entry):
            entry = future_to_entry[future]
            try:
                result = future.result()
                if result['is_relevant']:
                    entry['llm_result'] = result
                    relevant_news.append(entry)
                    news_summaries_for_aux.append(result['summary'])
                    print(f"  âœ… {entry['title'][:30]}...", flush=True)
                else:
                    # print(f"  âŒ {entry['title'][:30]}...", flush=True)
                    pass
            except Exception:
                pass
                
    # ã€å¼·åˆ¶æ¡ç”¨ãƒ­ã‚¸ãƒƒã‚¯ã€‘é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒ0ä»¶ã®å ´åˆã€å€™è£œã‹ã‚‰å¼·åˆ¶çš„ã«1ã¤é¸ã¶
    if not relevant_news and candidates:
        print("  âš ï¸ é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒ0ä»¶ã®ãŸã‚ã€å¼·åˆ¶æ¡ç”¨ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™...", flush=True)
        forced_news = force_pick_news(candidates, stock_info)
        if forced_news:
            relevant_news.append(forced_news)
            news_summaries_for_aux.append(forced_news['llm_result']['summary'])

    print(f"âœ… é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(relevant_news)}ä»¶", flush=True)
    
    # 3. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆv5.1æ©Ÿèƒ½ï¼‰
    clustered_news = []
    if relevant_news:
        # å¼•æ•°é †åºä¿®æ­£: (stock_name, relevant_news)
        clusters = cluster_news_by_topic(stock_info['name'], relevant_news)
        print_clustering_log(stock_info['name'], clusters)
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ã‚¯ãƒ©ã‚¹ã‚¿æ§‹é€ ã‚’æœŸå¾…ã—ã¦ã„ã‚‹ãŸã‚ã€prepare_delivery_newsã‚’é€šã•ãšç›´æ¥æ¸¡ã™
        clustered_news = clusters['clusters']
        print(f"âœ… é…ä¿¡: {len(clustered_news)}ã‚¯ãƒ©ã‚¹ã‚¿", flush=True)
    
    # 4. æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”Ÿæˆï¼ˆv5.3æ–°æ©Ÿèƒ½ï¼‰
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªãã¦ã‚‚æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚‹ã®ã§å¿…ãšç”Ÿæˆã™ã‚‹
    print(f"ğŸ“‰ æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...", flush=True)
    
    # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—
    price_data = get_stock_price_data(stock_code)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    investment_aux = generate_investment_aux_news(
        stock_code, 
        stock_info['name'], 
        price_data, 
        news_summaries_for_aux
    )
    print(f"âœ… æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†", flush=True)
    
    return {
        'stock_code': stock_code,
        'stock_name': stock_info['name'],
        'news': clustered_news,
        'investment_aux': investment_aux, # è¿½åŠ 
        'news_count': len(relevant_news)
    }

def main():
    print("============================================================", flush=True)
    print(f"å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  {VERSION}", flush=True)
    print("============================================================", flush=True)
    
    # ç¬¬1æ®µéš: ç›´è¿‘7æ—¥
    print("\n=== ç¬¬1æ®µéš: ç›´è¿‘7æ—¥ãƒ¢ãƒ¼ãƒ‰ ===", flush=True)
    entries = fetch_rss_feeds(days_back=7)
    entries = resolve_redirects(entries)
    
    results = {}
    stocks_needing_fallback = []
    
    for stock_code, stock_info in STOCKS.items():
        result = process_stock_news(stock_code, stock_info, entries, is_fallback_mode=False)
        results[stock_code] = result
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒ0ä»¶ã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾è±¡ã«è¿½åŠ 
        # â€»å¼·åˆ¶æ¡ç”¨ãƒ­ã‚¸ãƒƒã‚¯ãŒå…¥ã£ãŸã®ã§ã€candidatesãŒ0ä»¶ã®å ´åˆã®ã¿ã“ã“ã«æ¥ã‚‹ã¯ãš
        if result['news_count'] == 0:
            stocks_needing_fallback.append(stock_code)
            
    # ç¬¬2æ®µéš: 30æ—¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹0ä»¶ã®éŠ˜æŸ„ã®ã¿ï¼‰
    if stocks_needing_fallback:
        print("\n=== ç¬¬2æ®µéš: 30æ—¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ ===", flush=True)
        print(f"å¯¾è±¡éŠ˜æŸ„: {', '.join(stocks_needing_fallback)}", flush=True)
        
        # éå»30æ—¥åˆ†ã®RSSã‚’å–å¾—
        fallback_entries = fetch_rss_feeds(days_back=30)
        fallback_entries = resolve_redirects(fallback_entries)
        
        for stock_code in stocks_needing_fallback:
            stock_info = STOCKS[stock_code]
            print(f"ğŸ”„ {stock_info['name']} ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’é–‹å§‹...", flush=True)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å†å‡¦ç†
            result = process_stock_news(stock_code, stock_info, fallback_entries, is_fallback_mode=True)
            
            # çµæœã‚’ä¸Šæ›¸ã
            results[stock_code] = result

    # ãƒ¡ãƒ¼ãƒ«ä½œæˆã¨é€ä¿¡
    print("\nğŸ“§ ãƒ¡ãƒ¼ãƒ«ä½œæˆä¸­...", flush=True)
    email_body = create_email_body(results)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    with open("email_preview.html", "w", encoding="utf-8") as f:
        f.write(email_body)
    print("  ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¿å­˜: email_preview.html", flush=True)
    
    print("ğŸš€ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ä¸­...", flush=True)
    status_code = send_email_via_sendgrid(
        api_key=SENDGRID_API_KEY,
        from_email=RECIPIENT_EMAIL,
        to_email=RECIPIENT_EMAIL,
        subject=f"ã€å°æ¹¾æ ªã€‘æœ¬æ—¥ã®æŠ•è³‡åˆ¤æ–­ãƒ¬ãƒãƒ¼ãƒˆ ({datetime.now(JST).strftime('%Y/%m/%d')})",
        html_content=email_body
    )
    
    if 200 <= status_code < 300:
        print("âœ… é€ä¿¡æˆåŠŸï¼", flush=True)
    else:
        print(f"âŒ é€ä¿¡å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {status_code}", flush=True)

if __name__ == "__main__":
    main()
