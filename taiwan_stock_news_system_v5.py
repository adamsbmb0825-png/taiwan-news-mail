#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  v5.3 (Restored & Enhanced)
- v5.2-liteã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ãƒ™ãƒ¼ã‚¹ã«å¾©å…ƒ
- æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ ªä¾¡ãƒ•ã‚§ãƒ¼ã‚ºåˆ†æï¼‰ã‚’è¿½åŠ 
- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ›´ãªã—ã§çµ±åˆ
"""

VERSION = "v5.3-restored-20260122"

import os
import feedparser
import requests
from delayed_valuable_news import is_delayed_valuable_news
from openai import OpenAI
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail
import json
import hashlib
import re
from datetime import datetime, timedelta
from dateutil import parser as date_parser
import pytz
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from news_clustering_v51 import cluster_news_by_topic, prepare_delivery_news, print_clustering_log
from investment_aux_generator import generate_investment_aux_news

# OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
client = OpenAI()

# å°æ¹¾æ™‚é–“
TW_TZ = pytz.timezone('Asia/Taipei')

# çµ±è¨ˆæƒ…å ±
STATS = {
    'cache_hit': 0,
    'cache_miss': 0,
    'redirect_timeout': 0,
    'redirect_failed': 0,
    'sns_domain_excluded': 0,
    'sns_publisher_excluded': 0,
    'duplicate_excluded': 0,
    'unknown_publisher_excluded': 0
}

# éŠ˜æŸ„æƒ…å ±ã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
def load_stocks():
    """stocks.jsonã‹ã‚‰éŠ˜æŸ„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open('stocks.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('stocks', {})
    except FileNotFoundError:
        print("ã‚¨ãƒ©ãƒ¼: stocks.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {}
    except json.JSONDecodeError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: stocks.json ã®å½¢å¼ãŒä¸æ­£ã§ã™: {e}", flush=True)
        return {}

STOCKS = load_stocks()

# RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆv5.2-lite: 30ä»¶ã«å‰Šæ¸›ã€å¤šé¢æ€§ç¶­æŒï¼‰
RSS_FEEDS = [
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘  éŠ˜æŸ„ç›´çµã‚¯ã‚¨ãƒªï¼ˆ10ä»¶ï¼‰
    # ========================================
    
    # å°ç©é›»ï¼ˆ2330ï¼‰ - 3ä»¶
    "https://news.google.com/rss/search?q=å°ç©é›»+OR+TSMC&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=TSMC&hl=en-US&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=TSMC&hl=ja&gl=JP&ceid=JP:ja",
    
    # å‰µè¦‹ï¼ˆ2451ï¼‰ - 2ä»¶
    "https://news.google.com/rss/search?q=å‰µè¦‹+OR+Transcend&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å‰µè¦‹+OR+Transcend&hl=ja&gl=JP&ceid=JP:ja",
    
    # å®‡ç»ï¼ˆ8271ï¼‰ - 2ä»¶
    "https://news.google.com/rss/search?q=å®‡ç»+OR+Apacer&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=Apacer&hl=en-US&gl=US&ceid=US:en",
    
    # å»£é”ï¼ˆ2382ï¼‰ - 3ä»¶
    "https://news.google.com/rss/search?q=å»£é”+OR+Quanta&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=Quanta+Computer&hl=en-US&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=å»£é”+OR+Quanta&hl=ja&gl=JP&ceid=JP:ja",
    
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘¡ ä¸Šæµãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚¯ã‚¨ãƒªï¼ˆ13ä»¶ï¼‰
    # ========================================
    
    # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ6ä»¶ï¼‰
    "https://news.google.com/rss/search?q=EUV&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=CoWoS&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=HBM&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=æ¶²å†·&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å…ˆé€²è£½ç¨‹&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å…ˆé€²å°è£&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # é¡§å®¢ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆ3ä»¶ï¼‰
    "https://news.google.com/rss/search?q=NVIDIA&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=AIä¼ºæœå™¨&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=GB200&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # æ”¿ç­–ãƒ»åœ°æ”¿å­¦ï¼ˆ2ä»¶ï¼‰
    "https://news.google.com/rss/search?q=ç¾åœ‹å» &hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=é—œç¨…&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # éœ€çµ¦ãƒ»ä¾›çµ¦åˆ¶ç´„ï¼ˆ2ä»¶ï¼‰
    "https://news.google.com/rss/search?q=DRAMåƒ¹æ ¼&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=ç”¢èƒ½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘¢ æ¥­ç¸¾ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã‚¯ã‚¨ãƒªï¼ˆ4ä»¶ï¼‰
    # ========================================
    
    "https://news.google.com/rss/search?q=å°ç©é›»+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å‰µè¦‹+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å®‡ç»+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å»£é”+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘£ å…±é€šæ¥­ç•Œã‚¯ã‚¨ãƒªï¼ˆ3ä»¶ï¼‰
    # ========================================
    
    "https://news.google.com/rss/search?q=åŠå°é«”&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=DRAM+OR+NAND&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=ODM&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

# SNSãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆ
SNS_DOMAINS = [
    'threads.net',
    'instagram.com',
    'line.me',
    'linkedin.com',
    'tiktok.com',
    'youtube.com', 'youtu.be'
]

def is_sns_domain(url):
    """URLãŒSNSãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    url_lower = url.lower()
    return any(sns in url_lower for sns in SNS_DOMAINS)

def clean_url(url):
    """URLã‹ã‚‰ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
    parsed = urlparse(url)
    query_params = parse_qs(parsed.query)
    
    # é™¤å¤–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    exclude_params = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
                      'fbclid', 'gclid', 'msclkid', 'oc', '_ga', '_gl']
    
    # ã‚¯ãƒªãƒ¼ãƒ³ãªã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½œæˆ
    clean_params = {k: v for k, v in query_params.items() if k not in exclude_params}
    clean_query = urlencode(clean_params, doseq=True)
    
    # URLã‚’å†æ§‹ç¯‰
    clean_parsed = parsed._replace(query=clean_query)
    return urlunparse(clean_parsed)

def resolve_final_url(url, timeout=2):
    """
    ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’è¿½è·¡ã—ã¦æœ€çµ‚åˆ°é”URLã‚’å–å¾—
    ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 2ç§’ï¼ˆv5.2-liteã§çŸ­ç¸®ï¼‰
    """
    try:
        response = requests.head(url, allow_redirects=True, timeout=timeout)
        final_url = clean_url(response.url)
        return final_url
    except requests.Timeout:
        STATS['redirect_timeout'] += 1
        return None
    except Exception as e:
        STATS['redirect_failed'] += 1
        return None

def extract_publisher_from_url(url):
    """URLã‹ã‚‰å‡ºå…¸ãƒ¡ãƒ‡ã‚£ã‚¢åã‚’æŠ½å‡º"""
    domain_mapping = {
        'cnyes.com': 'é‰…äº¨ç¶²',
        'ctee.com.tw': 'å·¥å•†æ™‚å ±',
        'technews.tw': 'TechNews ç§‘æŠ€æ–°å ±',
        'udn.com': 'è¯åˆæ–°èç¶²',
        'ltn.com.tw': 'è‡ªç”±æ™‚å ±',
        'chinatimes.com': 'ä¸­æ™‚æ–°èç¶²',
        'cna.com.tw': 'ä¸­å¤®ç¤¾ CNA',
        'moneydj.com': 'MoneyDJ',
        'eettaiwan.com': 'EE Times Taiwan',
        'digitimes.com.tw': 'DIGITIMES',
        'storm.mg': 'é¢¨å‚³åª’',
        'businessweekly.com.tw': 'å•†æ¥­å‘¨åˆŠ',
        'cw.com.tw': 'å¤©ä¸‹é›œèªŒ',
        'wealth.com.tw': 'è²¡è¨Š',
        'mirrormedia.mg': 'é¡é€±åˆŠ',
        'ettoday.net': 'ETtoday',
        'setn.com': 'ä¸‰ç«‹æ–°èç¶²',
        'nownews.com': 'NOWnews',
        'yahoo.com': 'Yahooå¥‡æ‘©',
        'pchome.com.tw': 'PChome',
        'cmoney.tw': 'CMoney',
        'moneysmart.tw': 'MoneySmart',
        'wealth.com.tw': 'è²¡è¨Š',
        'businesstoday.com.tw': 'ä»Šå‘¨åˆŠ',
        'smart.businessweekly.com.tw': 'Smartè‡ªå­¸ç¶²',
        'money-link.com.tw': 'ç†è²¡å‘¨åˆŠ',
        'moneyweekly.com.tw': 'ç†è²¡å‘¨åˆŠ',
        'ctee.com.tw': 'å·¥å•†æ™‚å ±',
        'economic.com.tw': 'ç¶“æ¿Ÿæ—¥å ±',
        'appledaily.com.tw': 'è˜‹æœæ–°èç¶²',
        'ctwant.com': 'CTWANT',
        'sinotrade.com.tw': 'æ°¸è±é‡‘è­‰åˆ¸',
        'wantgoo.com': 'ç©è‚¡ç¶²',
        'wantrich.chinatimes.com': 'æ—ºå¾—å¯Œç†è²¡ç¶²',
        'knowing.asia': 'knowing',
        'newtalk.tw': 'æ–°é ­æ®¼',
        'taiwannews.com.tw': 'Taiwan News',
        'rti.org.tw': 'ä¸­å¤®å»£æ’­é›»å°',
        'epochtimes.com': 'å¤§ç´€å…ƒ',
        'ntdtv.com': 'æ–°å”äºº',
        'voacantonese.com': 'ç¾åœ‹ä¹‹éŸ³',
        'rfi.fr': 'RFI',
        'bbc.com': 'BBC',
        'reuters.com': 'Reuters',
        'bloomberg.com': 'Bloomberg',
        'ft.com': 'Financial Times',
        'wsj.com': 'Wall Street Journal',
        'nikkei.com': 'æ—¥ç¶“ä¸­æ–‡ç¶²',
    }
    
    parsed = urlparse(url)
    domain = parsed.netloc.lower().replace('www.', '')
    
    for key, value in domain_mapping.items():
        if key in domain:
            return value
    
    return None

def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ï¼ˆå…¨è§’/åŠè§’çµ±ä¸€ã€è¨˜å·é™¤å»ã€ç©ºç™½åœ§ç¸®ï¼‰"""
    # å…¨è§’â†’åŠè§’
    text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
    # è¨˜å·é™¤å»
    text = re.sub(r'[^\w\s]', '', text)
    # ç©ºç™½åœ§ç¸®
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def generate_article_signature(title, publisher, pub_date, snippet):
    """è¨˜äº‹ç½²åã‚’ç”Ÿæˆï¼ˆé‡è¤‡æ’é™¤ç”¨ï¼‰"""
    normalized_title = normalize_text(title)
    normalized_snippet = normalize_text(snippet[:120])
    date_str = pub_date.strftime('%Y-%m-%d') if pub_date else 'unknown'
    
    signature_string = f"{normalized_title}|{publisher}|{date_str}|{normalized_snippet}"
    return hashlib.md5(signature_string.encode('utf-8')).hexdigest()

def load_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    try:
        with open('.taiwan_stock_news_cache_v5.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"news": {}, "topics": {}}

def save_cache(cache):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    with open('.taiwan_stock_news_cache_v5.json', 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def clean_cache(cache):
    """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    now = datetime.now(TW_TZ)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥: 30æ—¥é–“ä¿æŒ
    if 'news' not in cache:
        cache['news'] = {}
    news_cutoff = (now - timedelta(days=30)).isoformat()
    cache['news'] = {sig: data for sig, data in cache['news'].items() 
                     if data.get('cached_at', '') > news_cutoff}
    
    # è«–ç‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥: 7å–¶æ¥­æ—¥ï¼ˆç´„10æ—¥ï¼‰ä¿æŒ
    if 'topics' not in cache:
        cache['topics'] = {}
    topic_cutoff = (now - timedelta(days=10)).isoformat()
    cache['topics'] = {stock_id: data for stock_id, data in cache['topics'].items() 
                       if data.get('cached_at', '') > topic_cutoff}
    
    return cache

def process_rss_entry(entry, cache):
    """
    RSSã‚¨ãƒ³ãƒˆãƒªã‚’å‡¦ç†ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¿½è·¡ã‚’ã‚¹ã‚­ãƒƒãƒ—
    """
    rss_url = entry.get("link", "")
    title = entry.get("title", "")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆrss_urlãƒ™ãƒ¼ã‚¹ï¼‰
    # æ³¨: å³å¯†ã«ã¯URLè§£æ±ºå¾Œã®URLã§ãƒã‚§ãƒƒã‚¯ã™ã¹ãã ãŒã€é«˜é€ŸåŒ–ã®ãŸã‚ã“ã“ã§ä¸€æ¬¡ãƒã‚§ãƒƒã‚¯
    for sig, data in cache['news'].items():
        if data.get('url') == rss_url:
            STATS['cache_hit'] += 1
            return data

    # URLè§£æ±º
    final_url = resolve_final_url(rss_url)
    if not final_url:
        return None
        
    # SNSãƒ‰ãƒ¡ã‚¤ãƒ³é™¤å¤–
    if is_sns_domain(final_url):
        STATS['sns_domain_excluded'] += 1
        return None

    # å‡ºç‰ˆç¤¾æŠ½å‡º
    publisher = extract_publisher_from_url(final_url)
    if not publisher:
        # Google Newsã®å ´åˆã€sourceã‚¿ã‚°ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        if 'source' in entry:
            publisher = entry.source.get('title')
        
        if not publisher:
            STATS['unknown_publisher_excluded'] += 1
            return None

    # æ—¥ä»˜è§£æ
    pub_date = None
    if "published" in entry:
        try:
            pub_date = date_parser.parse(entry.published).astimezone(TW_TZ)
        except:
            pass
    
    if not pub_date:
        pub_date = datetime.now(TW_TZ)

    # ç½²åç”Ÿæˆã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ï¼‰
    snippet = entry.get("summary", "")
    signature = generate_article_signature(title, publisher, pub_date, snippet)
    
    if signature in cache['news']:
        STATS['cache_hit'] += 1
        return cache['news'][signature]

    STATS['cache_miss'] += 1
    
    # æ–°è¦ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    news_item = {
        "title": title,
        "url": final_url,
        "publisher": publisher,
        "date": pub_date.isoformat(),
        "snippet": snippet,
        "signature": signature,
        "cached_at": datetime.now(TW_TZ).isoformat()
    }
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°ï¼ˆå‘¼ã³å‡ºã—å…ƒã§ä¿å­˜ãŒå¿…è¦ï¼‰
    cache['news'][signature] = news_item
    
    return news_item

def collect_news_from_rss(days=7):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰"""
    print(f"ğŸ“° RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­... (éå»{days}æ—¥åˆ†)")
    
    all_entries = []
    cutoff_date = datetime.now(TW_TZ) - timedelta(days=days)
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ï¼ˆI/Oãƒã‚¦ãƒ³ãƒ‰ãªã®ã§ã‚¹ãƒ¬ãƒƒãƒ‰æ•°å¤šã‚ã§ã‚‚OKã ãŒã€ç›¸æ‰‹å…ˆè² è·è€ƒæ…®ã—åˆ¶é™ï¼‰
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(feedparser.parse, url) for url in RSS_FEEDS]
        
        for future in as_completed(futures):
            feed = future.result()
            for entry in feed.entries:
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä¸€æ¬¡ï¼‰
                if "published" in entry:
                    try:
                        pub_date = date_parser.parse(entry.published).astimezone(TW_TZ)
                        if pub_date < cutoff_date:
                            continue
                    except:
                        pass
                all_entries.append(entry)
    
    print(f"  RSSåé›†å®Œäº†: {len(all_entries)}ä»¶")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
    cache = load_cache()
    cache = clean_cache(cache)
    
    processed_news = []
    
    # URLè§£æ±ºã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
    print(f"ğŸ”— URLè§£æ±ºä¸­ï¼ˆ{len(all_entries)}ä»¶ï¼‰...")
    
    # å‡¦ç†ä¸Šé™è¨­å®šï¼ˆAPIã‚³ã‚¹ãƒˆã¨æ™‚é–“ç¯€ç´„ï¼‰
    MAX_URL_PROCESS = 200
    if len(all_entries) > MAX_URL_PROCESS:
        print(f"  âš ï¸ ä»¶æ•°ãŒå¤šã„ãŸã‚ã€æœ€æ–°{MAX_URL_PROCESS}ä»¶ã®ã¿å‡¦ç†ã—ã¾ã™")
        all_entries = all_entries[:MAX_URL_PROCESS]

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(process_rss_entry, entry, cache) for entry in all_entries]
        
        for future in as_completed(futures):
            result = future.result()
            if result:
                processed_news.append(result)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    save_cache(cache)
    
    # é‡è¤‡æ’é™¤ï¼ˆURLãƒ™ãƒ¼ã‚¹ï¼‰
    unique_news = []
    seen_urls = set()
    
    for news in processed_news:
        if news['url'] not in seen_urls:
            unique_news.append(news)
            seen_urls.add(news['url'])
        else:
            STATS['duplicate_excluded'] += 1
            
    print(f"âœ… é‡è¤‡é™¤å¤–å¾Œ: {len(unique_news)}ä»¶")
    return unique_news

def filter_news_by_stock(news_list, stock_id, stock_info):
    """éŠ˜æŸ„ã«é–¢é€£ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒï¼‰"""
    keywords = stock_info.get('keywords', [])
    stock_name = stock_info.get('name', '')
    
    # éŠ˜æŸ„åã‚‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«è¿½åŠ 
    search_keywords = keywords + [stock_name, stock_id]
    
    relevant_news = []
    for news in news_list:
        text = (news['title'] + " " + news['snippet']).lower()
        if any(k.lower() in text for k in search_keywords):
            relevant_news.append(news)
            
    return relevant_news

def process_stock_news(stock_id, stock_info, all_news, cache, fallback_mode=False):
    """
    éŠ˜æŸ„ã”ã¨ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å‡¦ç†ãƒ•ãƒ­ãƒ¼
    1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
    2. LLMé–¢é€£æ€§åˆ¤å®šï¼ˆå³é¸ï¼‰
    3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ»è¦ç´„
    4. æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”Ÿæˆãƒ»è¿½åŠ ï¼ˆæ–°è¦ï¼‰
    """
    print(f"============================================================")
    print(f"ğŸ“Š {stock_info['name']}ï¼ˆ{stock_id}ï¼‰")
    print(f"============================================================")
    
    # 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
    candidates = filter_news_by_stock(all_news, stock_id, stock_info)
    print(f"å€™è£œãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(candidates)}ä»¶")
    
    if not candidates:
        print("  âŒ å€™è£œãªã—")
        return None

    # 2. LLMé–¢é€£æ€§åˆ¤å®šï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ä»¶æ•°åˆ¶é™ï¼‰
    MAX_LLM_CHECK = 15 if not fallback_mode else 30
    if len(candidates) > MAX_LLM_CHECK:
        # æ—¥ä»˜ãŒæ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã®ã¿ãƒã‚§ãƒƒã‚¯
        candidates.sort(key=lambda x: x['date'], reverse=True)
        candidates = candidates[:MAX_LLM_CHECK]
    
    relevant_news = []
    
    # é…å»¶ä¾¡å€¤åˆ¤å®šï¼ˆdelayed_valuable_news.pyï¼‰ã‚’ä½¿ç”¨
    # LLMåˆ¤å®šã¯ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŸã‚ã€ã“ã“ã‚‚ä¸¦åˆ—åŒ–ã—ãŸã„ãŒã€
    # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆè€ƒæ…®ã—ã¦ç›´åˆ—å®Ÿè¡Œï¼ˆã¾ãŸã¯å°‘æ•°ã®ä¸¦åˆ—ï¼‰
    
    # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿åˆ¤å®šçµæœãŒã‚ã‚Œã°åˆ©ç”¨
    # ï¼ˆä»Šå›ã¯ç°¡æ˜“å®Ÿè£…ã¨ã—ã¦ã€news_clustering_v51.py å†…ã®ãƒ­ã‚¸ãƒƒã‚¯ã«ä»»ã›ã‚‹ã‹ã€
    #   ã“ã“ã§è‡ªå‰ã§å‘¼ã¶ã‹ã€‚v5.2-liteã§ã¯ã“ã“ã§å‘¼ã¶è¨­è¨ˆï¼‰
    
    for news in candidates:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼: signature + stock_id
        cache_key = f"{news['signature']}_{stock_id}_relevance"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ã‚Œã°ä½¿ã†ï¼ˆåˆ¤å®šçµæœã¯å¤‰ã‚ã‚‰ãªã„ã¯ãšï¼‰
        # â€»å®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€ã“ã“ã§ã¯æ¯å›åˆ¤å®šï¼ˆdelayed_valuable_newså†…ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿæ§‹ã‚ã‚Œã°ã‚ˆã„ãŒï¼‰
        # ä»Šå›ã¯ç›´æ¥ is_delayed_valuable_news ã‚’å‘¼ã¶
        
        is_relevant, reason = is_delayed_valuable_news(news, stock_id, stock_info)
        
        if is_relevant:
            news['relevance_reason'] = reason
            relevant_news.append(news)
            # print(f"  âœ… é–¢é€£ã‚ã‚Š: {news['title'][:20]}...")
        else:
            pass
            # print(f"  ğŸ—‘ï¸ é™¤å¤–: {news['title'][:20]}...")

    print(f"âœ… é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(relevant_news)}ä»¶")
    
    if not relevant_news:
        return None

    # 3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ»è¦ç´„ï¼ˆv5.1ã®ãƒ­ã‚¸ãƒƒã‚¯å†åˆ©ç”¨ï¼‰
    # ã“ã“ã§æ—¥æœ¬èªç¿»è¨³ã¨è¦ç´„ãŒè¡Œã‚ã‚Œã‚‹
    clustered_news = cluster_news_by_topic(relevant_news, stock_id, stock_info)
    
    # 4. æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”Ÿæˆãƒ»è¿½åŠ ï¼ˆæ–°è¦ï¼‰
    # æ—¢å­˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã®æœ«å°¾ã«ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿½åŠ ã™ã‚‹
    try:
        aux_news = generate_investment_aux_news(stock_id, stock_info, relevant_news)
        if aux_news:
            # æ—¢å­˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å½¢å¼ã«åˆã‚ã›ã‚‹
            formatted_aux_news = {
                "topic_theme": "ğŸ“‰ æŠ•è³‡åˆ¤æ–­è£œåŠ©ï¼ˆæ ªä¾¡ãƒ•ã‚§ãƒ¼ã‚ºæ•´ç†ï¼‰",
                "title_ja": f"ã€{aux_news['phase']}ã€‘{aux_news['price_movement']}",
                "title_tw": "Market Phase Analysis", # ç¹ä½“å­—ã‚¿ã‚¤ãƒˆãƒ«ã¯è‹±èªè¡¨è¨˜ã§ä»£ç”¨ï¼ˆã¾ãŸã¯ç©ºæ–‡å­—ï¼‰
                "summary_ja": f"{aux_news['news_correlation']}\n\nğŸ’¡ æ³¨æ„ç‚¹: {aux_news['caution_point']}",
                "representative_reason": aux_news['news_correlation'], # åˆ†æãƒœãƒƒã‚¯ã‚¹ç”¨
                "source": "Market Analysis",
                "pub_date": datetime.now(TW_TZ).strftime('%Y-%m-%d %H:%M'),
                "url": "#", # ãƒªãƒ³ã‚¯ãªã—
                "related_score": 0, # ã‚¹ã‚³ã‚¢ãªã—
                "sentiment": "neutral"
            }
            clustered_news.append(formatted_aux_news)
            print("  âœ… æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"  âš ï¸ æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    return {
        'stock_id': stock_id,
        'stock_name': stock_info['name'],
        'news': clustered_news
    }

def main():
    print(f"ğŸš€ å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  {VERSION} èµ·å‹•")
    start_time = time.time()
    
    # 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆéå»7æ—¥ï¼‰
    all_news = collect_news_from_rss(days=7)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ï¼ˆãƒ—ãƒ­ã‚»ã‚¹å†…ã§å…±æœ‰ï¼‰
    cache = load_cache()
    
    results = {}
    
    # 2. éŠ˜æŸ„ã”ã¨ã«å‡¦ç†
    for stock_id, stock_info in STOCKS.items():
        # _commentãªã©ã¯ã‚¹ã‚­ãƒƒãƒ—
        if stock_id.startswith('_') or stock_id == 'stocks':
            continue
            
        res = process_stock_news(stock_id, stock_info, all_news, cache)
        
        if res:
            results[stock_id] = res
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ï¼ˆéå»30æ—¥ï¼‰
            print(f"âš ï¸ {stock_info['name']}: ç›´è¿‘7æ—¥é–“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰(30æ—¥)ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
            
            # 30æ—¥åˆ†ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å†åé›†ï¼ˆå…¨ä»¶ã¯é‡ã„ã®ã§ã€å¯¾è±¡éŠ˜æŸ„ã®ã‚¯ã‚¨ãƒªã ã‘å©ãã®ãŒç†æƒ³ã ãŒã€
            # å®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€æ—¢å­˜é–¢æ•°ã§30æ—¥æŒ‡å®šã§å†åé›†ã€‚ãŸã ã—ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ãã®ã§2å›ç›®ã¯é€Ÿã„ï¼‰
            # â€»æœ€é©åŒ–: æœ¬å½“ã¯ã“ã“ã§ã€Œã“ã®éŠ˜æŸ„å°‚ç”¨ã®RSSã€ã ã‘å©ãã¹ãã ãŒã€
            # RSS_FEEDSãƒªã‚¹ãƒˆãŒãƒ•ãƒ©ãƒƒãƒˆãªã®ã§ã€å…¨ä»¶å©ã„ã¦ã—ã¾ã†ã€‚
            # v5.2-liteã§ã¯è¨±å®¹ç¯„å›²ï¼ˆ30ä»¶ç¨‹åº¦ãªã‚‰ï¼‰
            
            fallback_news = collect_news_from_rss(days=30)
            res = process_stock_news(stock_id, stock_info, fallback_news, cache, fallback_mode=True)
            
            if res:
                results[stock_id] = res
            else:
                print(f"âŒ {stock_info['name']}: 30æ—¥é–“ã§ã‚‚é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—")
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—ã§ã‚‚æŠ•è³‡åˆ¤æ–­è£œåŠ©ã ã‘ã¯å‡ºã—ãŸã„å ´åˆã€ã“ã“ã§ç”Ÿæˆã™ã‚‹æ‰‹ã‚‚ã‚ã‚‹ãŒã€
                # ä»Šå›ã®è¦ä»¶ã¯ã€Œä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹0ä»¶ã‚’é˜²ãã€ã§ã¯ãªãã€Œä»¥å‰ã®æŒ™å‹•ã«æˆ»ã™ã€ãªã®ã§ã€
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã‘ã‚Œã°ãƒ¡ãƒ¼ãƒ«ã«ã‚‚è¼‰ã›ãªã„ï¼ˆã¾ãŸã¯ç©ºã§è¼‰ã›ã‚‹ï¼‰
                # ãŸã ã—ã€æŠ•è³‡åˆ¤æ–­è£œåŠ©ã¯ã€Œå¿…ãš1æœ¬ã€ã¨ã„ã†è¦ä»¶ãŒã‚ã‚‹ãŸã‚ã€
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªãã¦ã‚‚æŠ•è³‡åˆ¤æ–­è£œåŠ©ã ã‘ç”Ÿæˆã—ã¦è¿”ã™
                try:
                    aux_news = generate_investment_aux_news(stock_id, stock_info, [])
                    if aux_news:
                        formatted_aux_news = {
                            "topic_theme": "ğŸ“‰ æŠ•è³‡åˆ¤æ–­è£œåŠ©ï¼ˆæ ªä¾¡ãƒ•ã‚§ãƒ¼ã‚ºæ•´ç†ï¼‰",
                            "title_ja": f"ã€{aux_news['phase']}ã€‘{aux_news['price_movement']}",
                            "title_tw": "Market Phase Analysis",
                            "summary_ja": f"{aux_news['news_correlation']}\n\nğŸ’¡ æ³¨æ„ç‚¹: {aux_news['caution_point']}",
                            "representative_reason": aux_news['news_correlation'],
                            "source": "Market Analysis",
                            "pub_date": datetime.now(TW_TZ).strftime('%Y-%m-%d %H:%M'),
                            "url": "#",
                            "related_score": 0,
                            "sentiment": "neutral"
                        }
                        results[stock_id] = {
                            'stock_id': stock_id,
                            'stock_name': stock_info['name'],
                            'news': [formatted_aux_news]
                        }
                        print("  âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—ã®ãŸã‚ã€æŠ•è³‡åˆ¤æ–­è£œåŠ©ã®ã¿ç”Ÿæˆã—ã¾ã—ãŸ")
                except Exception as e:
                    print(f"  âš ï¸ æŠ•è³‡åˆ¤æ–­è£œåŠ©ç”Ÿæˆã‚¨ãƒ©ãƒ¼(ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯): {e}")

    # 3. ãƒ¡ãƒ¼ãƒ«ä½œæˆãƒ»é€ä¿¡
    if results:
        from email_template_v5 import create_email_body
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ¸¡ã™ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        # email_template_v5.py ã¯ {stock_id: {'news': [...]}} ã‚’æœŸå¾…ã—ã¦ã„ã‚‹ã¯ãš
        # ç¢ºèª: email_template_v5.py ã® create_email_body(news_data)
        
        html_content = create_email_body(results)
        
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¿å­˜
        with open('email_preview.html', 'w', encoding='utf-8') as f:
            f.write(html_content)
        print("ğŸ’¾ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ: email_preview.html")
        
        # é€ä¿¡
        recipient = os.environ.get('RECIPIENT_EMAIL')
        if recipient:
            message = Mail(
                from_email=recipient, # è‡ªåˆ†è‡ªèº«ã«é€ã‚‹ï¼ˆSendGrid Sender Identityå›é¿ï¼‰
                to_emails=recipient,
                subject=f"ğŸ‡¹ğŸ‡¼ å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ {datetime.now(TW_TZ).strftime('%Y/%m/%d')}",
                html_content=html_content
            )
            
            try:
                sg = SendGridAPIClient(os.environ.get('SENDGRID_API_KEY'))
                response = sg.send(message)
                print(f"âœ… é€ä¿¡æˆåŠŸï¼ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
            except Exception as e:
                print(f"âŒ é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            print("âš ï¸ RECIPIENT_EMAIL ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚é€ä¿¡ã‚¹ã‚­ãƒƒãƒ—")
            
    else:
        print("âŒ é…ä¿¡å¯¾è±¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    elapsed = time.time() - start_time
    print(f"â±ï¸ å‡¦ç†æ™‚é–“: {elapsed:.2f}ç§’")

if __name__ == "__main__":
    main()
