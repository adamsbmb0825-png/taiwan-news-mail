#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  v5.1
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¿½è·¡ã‚’ã‚¹ã‚­ãƒƒãƒ—
- ä¸¦åˆ—å‡¦ç†ã§é«˜é€ŸåŒ–
- çµ±è¨ˆãƒ­ã‚°ã§é€æ˜æ€§ã‚’ç¢ºä¿
- ãƒ‹ãƒ¥ãƒ¼ã‚¹å¤šæ§˜æ€§æ”¹å–„ï¼ˆè«–ç‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰
"""

VERSION = "v5.1-frozen-20260113-0320"

import os
import feedparser
import requests
from openai import OpenAI
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail
import json
import hashlib
import re
from datetime import datetime, timedelta
from dateutil import parser as date_parser
import pytz
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from news_clustering_v51 import cluster_news_by_topic, prepare_delivery_news, print_clustering_log

# OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
client = OpenAI()

# å°æ¹¾æ™‚é–“
TW_TZ = pytz.timezone('Asia/Taipei')

# çµ±è¨ˆæƒ…å ±
STATS = {
    'cache_hit': 0,
    'cache_miss': 0,
    'redirect_timeout': 0,
    'redirect_failed': 0,
    'sns_domain_excluded': 0,
    'sns_publisher_excluded': 0,
    'duplicate_excluded': 0,
    'unknown_publisher_excluded': 0
}

# éŠ˜æŸ„æƒ…å ±ã‚’å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
def load_stocks():
    """stocks.jsonã‹ã‚‰éŠ˜æŸ„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open('/home/ubuntu/stocks.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('stocks', {})
    except FileNotFoundError:
        print("ã‚¨ãƒ©ãƒ¼: stocks.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return {}
    except json.JSONDecodeError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: stocks.json ã®å½¢å¼ãŒä¸æ­£ã§ã™: {e}")
        return {}

STOCKS = load_stocks()

# RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆå»£é”å°‚ç”¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å¼·åŒ–ï¼‰
RSS_FEEDS = [
    # å°ç©é›»ãƒ»TSMCå°‚ç”¨
    "https://news.google.com/rss/search?q=å°ç©é›»+OR+TSMC&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # å‰µè¦‹ãƒ»å®‡ç»ãƒ»è¨˜æ†¶é«”å°‚ç”¨
    "https://news.google.com/rss/search?q=å‰µè¦‹+OR+å®‡ç»+OR+è¨˜æ†¶é«”&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # å®‡ç»å°‚ç”¨ï¼ˆå¼·åŒ–ï¼‰
    "https://news.google.com/rss/search?q=å®‡ç»+8271&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å®‡ç»+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=Apacer+è¨˜æ†¶é«”&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # å»£é”å°‚ç”¨ï¼ˆå¼·åŒ–ï¼‰
    "https://news.google.com/rss/search?q=å»£é”+2382&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å»£é”+AIä¼ºæœå™¨&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å»£é”+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # æ¥­ç•Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    "https://news.google.com/rss/search?q=åŠå°é«”+OR+æ™¶åœ“ä»£å·¥&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=DRAM+OR+NAND&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=ODM+OR+ä¼ºæœå™¨&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
]

# SNSãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆ
SNS_DOMAINS = [
    'facebook.com', 'm.facebook.com', 'fb.watch', 'l.facebook.com',
    'twitter.com', 'x.com', 't.co',
    'threads.net',
    'instagram.com',
    'line.me',
    'linkedin.com',
    'tiktok.com',
    'youtube.com', 'youtu.be'
]

def is_sns_domain(url):
    """URLãŒSNSãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    url_lower = url.lower()
    return any(sns in url_lower for sns in SNS_DOMAINS)

def clean_url(url):
    """URLã‹ã‚‰ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
    parsed = urlparse(url)
    query_params = parse_qs(parsed.query)
    
    # é™¤å¤–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    exclude_params = ['utm_source', 'utm_medium', 'utm_campaign', 'utm_term', 'utm_content',
                      'fbclid', 'gclid', 'msclkid', 'oc', '_ga', '_gl']
    
    # ã‚¯ãƒªãƒ¼ãƒ³ãªã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½œæˆ
    clean_params = {k: v for k, v in query_params.items() if k not in exclude_params}
    clean_query = urlencode(clean_params, doseq=True)
    
    # URLã‚’å†æ§‹ç¯‰
    clean_parsed = parsed._replace(query=clean_query)
    return urlunparse(clean_parsed)

def resolve_final_url(url, timeout=3):
    """
    ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’è¿½è·¡ã—ã¦æœ€çµ‚åˆ°é”URLã‚’å–å¾—
    ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 3ç§’
    """
    try:
        response = requests.head(url, allow_redirects=True, timeout=timeout)
        final_url = clean_url(response.url)
        return final_url
    except requests.Timeout:
        STATS['redirect_timeout'] += 1
        return None
    except Exception as e:
        STATS['redirect_failed'] += 1
        return None

def extract_publisher_from_url(url):
    """URLã‹ã‚‰å‡ºå…¸ãƒ¡ãƒ‡ã‚£ã‚¢åã‚’æŠ½å‡º"""
    domain_mapping = {
        'cnyes.com': 'é‰…äº¨ç¶²',
        'ctee.com.tw': 'å·¥å•†æ™‚å ±',
        'technews.tw': 'TechNews ç§‘æŠ€æ–°å ±',
        'udn.com': 'è¯åˆæ–°èç¶²',
        'ltn.com.tw': 'è‡ªç”±æ™‚å ±',
        'chinatimes.com': 'ä¸­æ™‚æ–°èç¶²',
        'cna.com.tw': 'ä¸­å¤®ç¤¾ CNA',
        'moneydj.com': 'MoneyDJ',
        'eettaiwan.com': 'EE Times Taiwan',
        'digitimes.com.tw': 'DIGITIMES',
        'storm.mg': 'é¢¨å‚³åª’',
        'businessweekly.com.tw': 'å•†æ¥­å‘¨åˆŠ',
        'cw.com.tw': 'å¤©ä¸‹é›œèªŒ',
        'wealth.com.tw': 'è²¡è¨Š',
        'mirrormedia.mg': 'é¡é€±åˆŠ',
        'ettoday.net': 'ETtoday',
        'setn.com': 'ä¸‰ç«‹æ–°èç¶²',
        'nownews.com': 'NOWnews',
        'yahoo.com': 'Yahooå¥‡æ‘©',
        'pchome.com.tw': 'PChome',
        'cmoney.tw': 'CMoney',
        'moneysmart.tw': 'MoneySmart',
        'wealth.com.tw': 'è²¡è¨Š',
        'businesstoday.com.tw': 'ä»Šå‘¨åˆŠ',
        'smart.businessweekly.com.tw': 'Smartè‡ªå­¸ç¶²',
        'money-link.com.tw': 'ç†è²¡å‘¨åˆŠ',
        'moneyweekly.com.tw': 'ç†è²¡å‘¨åˆŠ',
        'ctee.com.tw': 'å·¥å•†æ™‚å ±',
        'economic.com.tw': 'ç¶“æ¿Ÿæ—¥å ±',
        'appledaily.com.tw': 'è˜‹æœæ–°èç¶²',
        'ctwant.com': 'CTWANT',
        'sinotrade.com.tw': 'æ°¸è±é‡‘è­‰åˆ¸',
        'wantgoo.com': 'ç©è‚¡ç¶²',
        'wantrich.chinatimes.com': 'æ—ºå¾—å¯Œç†è²¡ç¶²',
        'knowing.asia': 'knowing',
        'newtalk.tw': 'æ–°é ­æ®¼',
        'taiwannews.com.tw': 'Taiwan News',
        'rti.org.tw': 'ä¸­å¤®å»£æ’­é›»å°',
        'epochtimes.com': 'å¤§ç´€å…ƒ',
        'ntdtv.com': 'æ–°å”äºº',
        'voacantonese.com': 'ç¾åœ‹ä¹‹éŸ³',
        'rfi.fr': 'RFI',
        'bbc.com': 'BBC',
        'reuters.com': 'Reuters',
        'bloomberg.com': 'Bloomberg',
        'ft.com': 'Financial Times',
        'wsj.com': 'Wall Street Journal',
        'nikkei.com': 'æ—¥ç¶“ä¸­æ–‡ç¶²',
    }
    
    parsed = urlparse(url)
    domain = parsed.netloc.lower().replace('www.', '')
    
    for key, value in domain_mapping.items():
        if key in domain:
            return value
    
    return None

def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–ï¼ˆå…¨è§’/åŠè§’çµ±ä¸€ã€è¨˜å·é™¤å»ã€ç©ºç™½åœ§ç¸®ï¼‰"""
    # å…¨è§’â†’åŠè§’
    text = text.translate(str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789'))
    # è¨˜å·é™¤å»
    text = re.sub(r'[^\w\s]', '', text)
    # ç©ºç™½åœ§ç¸®
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def generate_article_signature(title, publisher, pub_date, snippet):
    """è¨˜äº‹ç½²åã‚’ç”Ÿæˆï¼ˆé‡è¤‡æ’é™¤ç”¨ï¼‰"""
    normalized_title = normalize_text(title)
    normalized_snippet = normalize_text(snippet[:120])
    date_str = pub_date.strftime('%Y-%m-%d') if pub_date else 'unknown'
    
    signature_string = f"{normalized_title}|{publisher}|{date_str}|{normalized_snippet}"
    return hashlib.md5(signature_string.encode('utf-8')).hexdigest()

def load_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    try:
        with open('.taiwan_stock_news_cache_v5.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"news": {}, "topics": {}}

def save_cache(cache):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    with open('.taiwan_stock_news_cache_v5.json', 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def clean_cache(cache):
    """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    now = datetime.now(TW_TZ)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥: 30æ—¥é–“ä¿æŒ
    if 'news' not in cache:
        cache['news'] = {}
    news_cutoff = (now - timedelta(days=30)).isoformat()
    cache['news'] = {sig: data for sig, data in cache['news'].items() 
                     if data.get('cached_at', '') > news_cutoff}
    
    # è«–ç‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥: 7å–¶æ¥­æ—¥ï¼ˆç´„10æ—¥ï¼‰ä¿æŒ
    if 'topics' not in cache:
        cache['topics'] = {}
    topic_cutoff = (now - timedelta(days=10)).isoformat()
    cache['topics'] = {stock_id: data for stock_id, data in cache['topics'].items() 
                       if data.get('cached_at', '') > topic_cutoff}
    
    return cache

def process_rss_entry(entry, cache):
    """
    RSSã‚¨ãƒ³ãƒˆãƒªã‚’å‡¦ç†ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¿½è·¡ã‚’ã‚¹ã‚­ãƒƒãƒ—
    """
    rss_url = entry.get("link", "")
    title = entry.get("title", "")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆrss_urlãƒ™ãƒ¼ã‚¹ï¼‰
    if rss_url in cache.get('url_to_signature', {}):
        signature = cache['url_to_signature'][rss_url]
        if signature in cache['news']:
            STATS['cache_hit'] += 1
            cached_data = cache['news'][signature]
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰SNSåˆ¤å®š
            if is_sns_domain(cached_data['final_url']):
                STATS['sns_domain_excluded'] += 1
                return None
            return cached_data
    
    STATS['cache_miss'] += 1
    
    # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¿½è·¡
    final_url = resolve_final_url(rss_url, timeout=3)
    
    # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆæœªè§£æ±ºã¯é™¤å¤–
    if not final_url:
        return None
    
    # SNSãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
    if is_sns_domain(final_url):
        STATS['sns_domain_excluded'] += 1
        return None
    
    # å‡ºå…¸æŠ½å‡º
    publisher = None
    if hasattr(entry, 'source') and hasattr(entry.source, 'title'):
        publisher = entry.source.title
    if not publisher:
        publisher = extract_publisher_from_url(final_url)
    
    # å‡ºå…¸ä¸æ˜ã¯é™¤å¤–
    if not publisher:
        STATS['unknown_publisher_excluded'] += 1
        return None
    
    # å‡ºå…¸ãŒSNSãƒ‰ãƒ¡ã‚¤ãƒ³ã®å ´åˆã‚‚é™¤å¤–
    if any(sns in publisher.lower() for sns in ['facebook', 'twitter', 'x.com', 'instagram', 'line', 'threads']):
        STATS['sns_publisher_excluded'] += 1
        return None
    
    # æ—¥æ™‚å–å¾—
    pub_date = None
    if hasattr(entry, 'published'):
        try:
            pub_date = date_parser.parse(entry.published).astimezone(TW_TZ)
        except:
            pass
    
    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆå–å¾—
    snippet = entry.get("summary", "")[:200]
    
    # è¨˜äº‹ç½²åç”Ÿæˆ
    signature = generate_article_signature(title, publisher, pub_date, snippet)
    
    news_entry = {
        "title": title,
        "rss_url": rss_url,
        "final_url": final_url,
        "link": final_url,
        "publisher": publisher,
        "published": pub_date.isoformat() if pub_date else None,
        "snippet": snippet,
        "signature": signature,
        "cached_at": datetime.now(TW_TZ).isoformat()
    }
    
    return news_entry

def collect_news_parallel():
    """
    RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¸¦åˆ—åé›†
    """
    print("ğŸ“° RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
    cache = load_cache()
    
    # url_to_signatureãƒãƒƒãƒ”ãƒ³ã‚°ã‚’åˆæœŸåŒ–
    if 'url_to_signature' not in cache:
        cache['url_to_signature'] = {}
    
    all_entries = []
    
    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’åé›†
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            all_entries.extend(feed.entries)
        except Exception as e:
            print(f"âš ï¸  RSSåé›†ã‚¨ãƒ©ãƒ¼: {feed_url} - {e}")
    
    print(f"  RSSåé›†å®Œäº†: {len(all_entries)}ä»¶")
    
    # ä¸¦åˆ—å‡¦ç†ã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆè¿½è·¡
    news_list = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(process_rss_entry, entry, cache): entry for entry in all_entries}
        
        for i, future in enumerate(as_completed(futures), 1):
            if i % 50 == 0:
                print(f"  å‡¦ç†ä¸­: {i}/{len(all_entries)}ä»¶")
            
            try:
                result = future.result()
                if result:
                    news_list.append(result)
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                    cache['news'][result['signature']] = result
                    cache['url_to_signature'][result['rss_url']] = result['signature']
            except Exception as e:
                pass
    
    # é‡è¤‡é™¤å¤–
    unique_news = {}
    for news in news_list:
        signature = news['signature']
        if signature in unique_news:
            STATS['duplicate_excluded'] += 1
        else:
            unique_news[signature] = news
    
    print(f"âœ… é‡è¤‡é™¤å¤–å¾Œ: {len(unique_news)}ä»¶")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    cache = clean_cache(cache)
    save_cache(cache)
    
    return list(unique_news.values())

def translate_title(title):
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å°æ¹¾ã®é‡‘èãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ç°¡æ½”ã§æ­£ç¢ºãªç¿»è¨³ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": f"ä»¥ä¸‹ã®å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„:\n\n{title}"}
            ],
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return title

def judge_relevance(stock_id, stock_name, news_list):
    """LLMã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é–¢é€£æ€§ã‚’åˆ¤å®š"""
    news_text = "\n\n".join([
        f"[{i+1}] {news['title']}\nå‡ºå…¸: {news['publisher']}\næ¦‚è¦: {news['snippet']}"
        for i, news in enumerate(news_list[:20])  # æœ€å¤§20ä»¶
    ])
    
    prompt = f"""
ã‚ãªãŸã¯å°æ¹¾æ ªã®æŠ•è³‡åˆ¤æ–­ã‚’æ”¯æ´ã™ã‚‹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚

éŠ˜æŸ„: {stock_name}ï¼ˆ{stock_id}ï¼‰
æ¥­æ…‹: {STOCKS[stock_id]['business_type']}

ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‹ã‚‰ã€ã“ã®éŠ˜æŸ„ã®æŠ•è³‡åˆ¤æ–­ã«æœ‰åŠ¹ãªæƒ…å ±ã‚’å«ã‚€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸åˆ¥ã—ã¦ãã ã•ã„ã€‚

ã€åˆ¤å®šåŸºæº–ã€‘
- é–¢é€£ã‚ã‚Š: æ¥­ç¸¾ã€å—æ³¨ã€æŠ€è¡“ã€å¸‚å ´å‹•å‘ãªã©æŠ•è³‡åˆ¤æ–­ã«ç›´æ¥å½±éŸ¿ã™ã‚‹æƒ…å ±
- é–¢é€£æ€§ä¸æ˜: æ¥­ç•Œå…¨èˆ¬ã®è©±é¡Œã§ã€éŠ˜æŸ„ã¸ã®å½±éŸ¿ãŒä¸æ˜ç¢º
- å‚è€ƒå¤–: ç„¡é–¢ä¿‚ã€ã¾ãŸã¯æŠ•è³‡åˆ¤æ–­ã«ç„¡ä¾¡å€¤

ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆ:
{news_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®å½¢å¼ã§JSONé…åˆ—ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„:
[
  {{"index": 1, "relevance": "é–¢é€£ã‚ã‚Š", "score": 85, "reason": "ç†ç”±"}},
  {{"index": 2, "relevance": "å‚è€ƒå¤–", "score": 20, "reason": "ç†ç”±"}}
]
"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å°æ¹¾æ ªã®æŠ•è³‡åˆ¤æ–­ã‚’æ”¯æ´ã™ã‚‹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        
        result_text = response.choices[0].message.content.strip()
        # JSONã‚’æŠ½å‡º
        json_match = re.search(r'\[.*\]', result_text, re.DOTALL)
        if json_match:
            judgments = json.loads(json_match.group())
            return judgments
        return []
    except Exception as e:
        print(f"âš ï¸  é–¢é€£æ€§åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        return []

def generate_topic(stock_id, stock_name, relevant_news):
    """è«–ç‚¹ã‚’ç”Ÿæˆ"""
    cache = load_cache()
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if stock_id in cache.get('topics', {}):
        cached_topic = cache['topics'][stock_id]
        cached_at = datetime.fromisoformat(cached_topic['cached_at'])
        if datetime.now(TW_TZ) - cached_at < timedelta(days=10):
            return cached_topic['topic']
    
    news_text = "\n\n".join([
        f"[{i+1}] {news['title']}\nå‡ºå…¸: {news['publisher']}\næ¦‚è¦: {news['snippet']}"
        for i, news in enumerate(relevant_news[:5])
    ])
    
    prompt = f"""
éŠ˜æŸ„: {stock_name}ï¼ˆ{stock_id}ï¼‰
æ¥­æ…‹: {STOCKS[stock_id]['business_type']}

ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ã€æŠ•è³‡å®¶ãŒã€Œä»Šå¾Œã©ã“ã‚’è¦‹ã‚‹ã¹ãã‹ã€ã¨ã„ã†è«–ç‚¹ã‚’1æ–‡ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ãƒ‹ãƒ¥ãƒ¼ã‚¹:
{news_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
è«–ç‚¹ã®ã¿ã‚’1æ–‡ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã€Œ2ãƒŠãƒè£½é€ æŠ€è¡“ã®é‡ç”£é–‹å§‹æ™‚æœŸã¨CoWoSå—æ³¨å¢—åŠ ãŒåç›Šæ‹¡å¤§ã«ã©ã†å½±éŸ¿ã™ã‚‹ã‹ãŒç„¦ç‚¹ã€ï¼‰
"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å°æ¹¾æ ªã®æŠ•è³‡åˆ¤æ–­ã‚’æ”¯æ´ã™ã‚‹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5
        )
        
        topic = response.choices[0].message.content.strip()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if 'topics' not in cache:
            cache['topics'] = {}
        cache['topics'][stock_id] = {
            'topic': topic,
            'cached_at': datetime.now(TW_TZ).isoformat()
        }
        save_cache(cache)
        
        return topic
    except Exception as e:
        print(f"âš ï¸  è«–ç‚¹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return "å¸‚å ´å‹•å‘ã¨æ¥­ç¸¾ã¸ã®å½±éŸ¿ã‚’æ³¨è¦–"

def send_email(results, taipei_time):
    """ãƒ¡ãƒ¼ãƒ«é€ä¿¡"""
    from email_template_v5 import generate_html_email
    
    html_content = generate_html_email(results, taipei_time)
    
    message = Mail(
        from_email='adamsbmb0825@gmail.com',
        to_emails='adamsbmb0825@gmail.com',
        subject=f'ğŸ‡¹ğŸ‡¼ å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ v5.0 - {taipei_time.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}',
        html_content=html_content
    )
    
    try:
        sg = SendGridAPIClient(os.environ.get('SENDGRID_API_KEY'))
        response = sg.send(message)
        print(f"âœ… ãƒ¡ãƒ¼ãƒ«é€ä¿¡æˆåŠŸï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}ï¼‰")
    except Exception as e:
        print(f"âŒ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

def print_stats():
    """çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›"""
    print("\n" + "="*60)
    print("ğŸ“Š çµ±è¨ˆæƒ…å ±")
    print("="*60)
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {STATS['cache_hit']}ä»¶")
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: {STATS['cache_miss']}ä»¶")
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {STATS['cache_hit'] / (STATS['cache_hit'] + STATS['cache_miss']) * 100:.1f}%" if (STATS['cache_hit'] + STATS['cache_miss']) > 0 else "N/A")
    print(f"ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {STATS['redirect_timeout']}ä»¶")
    print(f"ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå¤±æ•—: {STATS['redirect_failed']}ä»¶")
    print(f"SNSãƒ‰ãƒ¡ã‚¤ãƒ³é™¤å¤–: {STATS['sns_domain_excluded']}ä»¶")
    print(f"SNSå‡ºå…¸é™¤å¤–: {STATS['sns_publisher_excluded']}ä»¶")
    print(f"å‡ºå…¸ä¸æ˜é™¤å¤–: {STATS['unknown_publisher_excluded']}ä»¶")
    print(f"é‡è¤‡é™¤å¤–: {STATS['duplicate_excluded']}ä»¶")
    print("="*60 + "\n")

def main():
    import os
    
    print("="*60)
    print(f"å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  {VERSION}")
    print("="*60)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
    all_news = collect_news_parallel()
    
    # çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
    print_stats()
    
    # å„éŠ˜æŸ„ã®å‡¦ç†
    results = {}
    
    for stock_id, stock_info in STOCKS.items():
        print("="*60)
        print(f"ğŸ“Š {stock_info['name']}ï¼ˆ{stock_id}ï¼‰")
        print("="*60)
        
        # é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
        stock_keywords = [stock_info['name'], stock_id]
        # å®‡ç»ã®å ´åˆã¯Apacerã‚‚è¿½åŠ 
        if stock_id == '8271':
            stock_keywords.append('Apacer')
            stock_keywords.append('apacer')
        candidate_news = [news for news in all_news if any(kw in news['title'] or kw in news['snippet'] for kw in stock_keywords)]
        
        print(f"å€™è£œãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(candidate_news)}ä»¶")
        
        if len(candidate_news) == 0:
            print("âš ï¸  é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã—")
            continue
        
        # LLMé–¢é€£æ€§åˆ¤å®š
        judgments = judge_relevance(stock_id, stock_info['name'], candidate_news)
        
        # é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
        relevant_news = []
        for judgment in judgments:
            if judgment['relevance'] == 'é–¢é€£ã‚ã‚Š':
                idx = judgment['index'] - 1
                if idx < len(candidate_news):
                    news = candidate_news[idx].copy()
                    news['relevance_score'] = judgment['score']
                    news['relevance_reason'] = judgment['reason']
                    # ã‚¿ã‚¤ãƒˆãƒ«ç¿»è¨³
                    print(f"  [ç¿»è¨³ä¸­] {news['title'][:50]}...")
                    news['title_ja'] = translate_title(news['title'])
                    relevant_news.append(news)
        
        print(f"âœ… é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(relevant_news)}ä»¶")
        
        if len(relevant_news) == 0:
            continue
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        relevant_news.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # è«–ç‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        clustering_result = cluster_news_by_topic(stock_info['name'], relevant_news)
        print_clustering_log(stock_info['name'], clustering_result)
        
        # é…ä¿¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æº–å‚™ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ä»˜ãï¼‰
        delivery_news = prepare_delivery_news(clustering_result, max_clusters=3)
        
        print(f"âœ… é…ä¿¡: {len(delivery_news)}ã‚¯ãƒ©ã‚¹ã‚¿")
        
        # è«–ç‚¹ç”Ÿæˆ
        topic = generate_topic(stock_id, stock_info['name'], relevant_news)
        
        results[stock_id] = {
            'stock_info': stock_info,
            'topic': topic,
            'news': delivery_news,
            'is_single_event': clustering_result['is_single_event'],
            'event_description': clustering_result['event_description']
        }
    
    # ãƒ¡ãƒ¼ãƒ«é€ä¿¡
    if results:
        now_taipei = datetime.now(TW_TZ)
        print("ğŸ“§ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ä¸­...")
        send_email(results, now_taipei)
    else:
        print("âš ï¸  é…ä¿¡ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“")

if __name__ == "__main__":
    main()
