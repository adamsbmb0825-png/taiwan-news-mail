#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  v5.3
- 2æ®µéšãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹å¼ï¼ˆç›´è¿‘7æ—¥ -> 30æ—¥ï¼‰
- é…å»¶ä¾¡å€¤åˆ¤å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆ
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ï¼ˆv5.1ï¼‰
- æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ ªä¾¡ãƒ•ã‚§ãƒ¼ã‚ºæ•´ç†ï¼‰æ©Ÿèƒ½ï¼ˆv5.3æ–°æ©Ÿèƒ½ï¼‰
"""

import os
import sys
import json
import time
import feedparser
import requests
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from openai import OpenAI
from email_template_v5 import create_email_body, send_email_via_sendgrid
from news_clustering import cluster_news_by_topic, prepare_delivery_news, print_clustering_log
from delayed_valuable_news import is_delayed_but_valuable
from stock_price_analyzer import get_stock_price_data
from investment_aux_generator import generate_investment_aux_news

# ============================================================
# è¨­å®šãƒ»å®šæ•°
# ============================================================

VERSION = "v5.3-20260121"

# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®š (JST)
JST = timezone(timedelta(hours=9))

# APIã‚­ãƒ¼è¨­å®š
SENDGRID_API_KEY = os.environ.get("SENDGRID_API_KEY")
RECIPIENT_EMAIL = os.environ.get("RECIPIENT_EMAIL")

if not SENDGRID_API_KEY or not RECIPIENT_EMAIL:
    print("ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° SENDGRID_API_KEY ã¾ãŸã¯ RECIPIENT_EMAIL ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", flush=True)
    sys.exit(1)

# OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
client = OpenAI()

# éŠ˜æŸ„ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿
def load_stocks():
    try:
        with open('stocks.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            # "stocks" ã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ãã®ä¸­èº«ã‚’è¿”ã™ã€ãªã‘ã‚Œã°ãã®ã¾ã¾è¿”ã™ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
            return data.get('stocks', data)
    except FileNotFoundError:
        print("ã‚¨ãƒ©ãƒ¼: stocks.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", flush=True)
        return {}
    except json.JSONDecodeError as e:
        print(f"ã‚¨ãƒ©ãƒ¼: stocks.json ã®å½¢å¼ãŒä¸æ­£ã§ã™: {e}", flush=True)
        return {}

STOCKS = load_stocks()

# RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆv5.2-lite: 30ä»¶ã«å‰Šæ¸›ã€å¤šé¢æ€§ç¶­æŒï¼‰
RSS_FEEDS = [
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘  éŠ˜æŸ„ç›´çµã‚¯ã‚¨ãƒªï¼ˆ10ä»¶ï¼‰
    # ========================================
    
    # å°ç©é›»ï¼ˆ2330ï¼‰ - 3ä»¶
    "https://news.google.com/rss/search?q=å°ç©é›»+OR+TSMC&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=TSMC&hl=en-US&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=TSMC&hl=ja&gl=JP&ceid=JP:ja",
    
    # å‰µè¦‹ï¼ˆ2451ï¼‰ - 2ä»¶
    "https://news.google.com/rss/search?q=å‰µè¦‹+OR+Transcend&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å‰µè¦‹+OR+Transcend&hl=ja&gl=JP&ceid=JP:ja",
    
    # å®‡ç»ï¼ˆ8271ï¼‰ - 2ä»¶
    "https://news.google.com/rss/search?q=å®‡ç»+OR+Apacer&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=Apacer&hl=en-US&gl=US&ceid=US:en",
    
    # å»£é”ï¼ˆ2382ï¼‰ - 3ä»¶
    "https://news.google.com/rss/search?q=å»£é”+OR+Quanta&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=Quanta+Computer&hl=en-US&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=å»£é”+OR+Quanta&hl=ja&gl=JP&ceid=JP:ja",
    
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘¡ ä¸Šæµãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚¯ã‚¨ãƒªï¼ˆ13ä»¶ï¼‰
    # ========================================
    
    # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ6ä»¶ï¼‰
    "https://news.google.com/rss/search?q=EUV&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=CoWoS&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=HBM&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=æ¶²å†·&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å…ˆé€²è£½ç¨‹&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å…ˆé€²å°è£&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # é¡§å®¢ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆ3ä»¶ï¼‰
    "https://news.google.com/rss/search?q=NVIDIA&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=AIä¼ºæœå™¨&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=GB200&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # æ”¿ç­–ãƒ»åœ°æ”¿å­¦ï¼ˆ2ä»¶ï¼‰
    "https://news.google.com/rss/search?q=ç¾åœ‹å» &hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=é—œç¨…&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # éœ€çµ¦ãƒ»ä¾›çµ¦åˆ¶ç´„ï¼ˆ2ä»¶ï¼‰
    "https://news.google.com/rss/search?q=DRAMåƒ¹æ ¼&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=ç”¢èƒ½&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    
    # ========================================
    # ã‚«ãƒ†ã‚´ãƒªâ‘¢ æ¥­ç¸¾ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã‚¯ã‚¨ãƒªï¼ˆ4ä»¶ï¼‰
    # ========================================
    
    "https://news.google.com/rss/search?q=å°ç©é›»+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å‰µè¦‹+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å®‡ç»+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant",
    "https://news.google.com/rss/search?q=å»£é”+ç‡Ÿæ”¶&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
]

# é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š
EXCLUDED_DOMAINS = [
    'ptt.cc', 'dcard.tw', 'mobile01.com', 'facebook.com', 'instagram.com',
    'youtube.com', 'wikipedia.org', 'amazon.com', 'ruten.com.tw', 'shopee.tw'
]

EXCLUDED_KEYWORDS = [
    'è‚¡å¸‚çˆ†æ–™åŒå­¸æœƒ', 'è¨è«–å€', 'æ‡¶äººåŒ…', 'å„ªæƒ ', 'æŠ˜æ‰£', 'é–‹ç®±', 'è©•æ¸¬',
    'è¬ è¨€', 'å…«å¦', 'PTT', 'Dcard', 'Mobile01'
]

# ============================================================
# é–¢æ•°å®šç¾©
# ============================================================

def fetch_rss_feeds(days_back=7):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    all_entries = []
    seen_links = set()
    
    cutoff_date = datetime.now(JST) - timedelta(days=days_back)
    print(f"ğŸ“° RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­... (éå»{days_back}æ—¥åˆ†)", flush=True)
    
    for url in RSS_FEEDS:
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                published = None
                if hasattr(entry, 'published_parsed'):
                    published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc).astimezone(JST)
                
                if published and published < cutoff_date:
                    continue
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if entry.link in seen_links:
                    continue
                
                seen_links.add(entry.link)
                
                # å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡º
                all_entries.append({
                    'title': entry.title,
                    'link': entry.link,
                    'published': published,
                    'source': entry.source.title if hasattr(entry, 'source') else 'Unknown',
                    'summary': entry.summary if hasattr(entry, 'summary') else ''
                })
                
        except Exception as e:
            print(f"  âš ï¸ ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {url} - {e}", flush=True)
            
    print(f"  RSSåé›†å®Œäº†: {len(all_entries)}ä»¶", flush=True)
    return all_entries

def resolve_redirects(entries):
    """Google Newsã®çŸ­ç¸®URLã‚’å±•é–‹"""
    print(f"ğŸ”— URLè§£æ±ºä¸­ï¼ˆ{len(entries)}ä»¶ï¼‰...", flush=True)
    
    # ä»¶æ•°ãŒå¤šã„å ´åˆã¯æœ€æ–°ã®ã‚‚ã®ã«çµã‚‹
    if len(entries) > 100:
        print("  âš ï¸ ä»¶æ•°ãŒå¤šã„ãŸã‚ã€æœ€æ–°100ä»¶ã®ã¿å‡¦ç†ã—ã¾ã™", flush=True)
        entries = sorted(entries, key=lambda x: x['published'] or datetime.min, reverse=True)[:100]
    
    resolved_entries = []
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å†åˆ©ç”¨
    session = requests.Session()
    
    for entry in entries:
        try:
            # Google Newsã®ãƒªãƒ³ã‚¯ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
            if 'news.google.com' in entry['link']:
                # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå…ˆã‚’å–å¾—ï¼ˆé«˜é€ŸåŒ–ï¼‰
                response = session.head(entry['link'], allow_redirects=True, timeout=5)
                entry['link'] = response.url
            
            # é™¤å¤–ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
            domain = entry['link'].split('/')[2] if len(entry['link'].split('/')) > 2 else ''
            if any(ex in domain for ex in EXCLUDED_DOMAINS):
                continue
                
            resolved_entries.append(entry)
            
        except Exception:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒªãƒ³ã‚¯ã®ã¾ã¾è¿½åŠ ï¼ˆã¾ãŸã¯é™¤å¤–ï¼‰
            resolved_entries.append(entry)
            
    print(f"âœ… é‡è¤‡é™¤å¤–å¾Œ: {len(resolved_entries)}ä»¶", flush=True)
    return resolved_entries

def analyze_relevance_with_llm(entry, stock_code, stock_info, is_fallback_mode=False):
    """LLMã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é–¢é€£æ€§ã‚’åˆ¤å®š"""
    
    # åˆ¤å®šåŸºæº–ã®æ§‹ç¯‰
    criteria = f"""
    1. éŠ˜æŸ„ã€Œ{stock_info['name']}ã€({stock_code}) ã®æ¥­ç¸¾ã€è£½å“ã€æŠ€è¡“ã€å—æ³¨ã€ææºã«ç›´æ¥é–¢ä¿‚ã™ã‚‹ã‹ã€‚
    2. ç«¶åˆä»–ç¤¾ã‚„æ¥­ç•Œå…¨ä½“ã®å‹•å‘ã§ã€ã“ã®éŠ˜æŸ„ã«é‡å¤§ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã€‚
    3. å˜ãªã‚‹å¸‚æ³æ¦‚æ³ã‚„ã€åå‰ãŒå‡ºã¦ã„ã‚‹ã ã‘ã®è¨˜äº‹ã¯é™¤å¤–ã™ã‚‹ã€‚
    """
    
    if is_fallback_mode:
        criteria += """
    4. ã€é‡è¦ã€‘è¨˜äº‹ã®æ—¥ä»˜ãŒå¤ãã¦ã‚‚ã€ç¾åœ¨ã‚‚æœ‰åŠ¹ãªæƒ…å ±ï¼ˆæŠ€è¡“è§£èª¬ã€é•·æœŸå±•æœ›ã€æ§‹é€ çš„ãªå¤‰åŒ–ãªã©ï¼‰ã¯ã€Œé–¢é€£ã‚ã‚Šã€ã¨ã™ã‚‹ã€‚
    5. çŸ­æœŸçš„ãªæ ªä¾¡å¤‰å‹•ã‚„ã€ã™ã§ã«çµ‚äº†ã—ãŸã‚¤ãƒ™ãƒ³ãƒˆã®é€Ÿå ±ã¯é™¤å¤–ã™ã‚‹ã€‚
        """
    
    prompt = f"""
    ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãŒã€æŠ•è³‡å®¶ã«ã¨ã£ã¦ã€Œ{stock_info['name']}ã€ã®åˆ†æã«å½¹ç«‹ã¤é‡è¦ãªæƒ…å ±ã‚’å«ã‚“ã§ã„ã‚‹ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
    
    ã‚¿ã‚¤ãƒˆãƒ«: {entry['title']}
    ã‚½ãƒ¼ã‚¹: {entry['source']}
    æ¦‚è¦: {entry['summary']}
    
    åˆ¤å®šåŸºæº–:
    {criteria}
    
    JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
    {{
        "is_relevant": true/false,
        "reason": "åˆ¤å®šç†ç”±ï¼ˆæ—¥æœ¬èªã€50æ–‡å­—ä»¥å†…ï¼‰",
        "summary": "è¨˜äº‹ã®è¦ç´„ï¼ˆæ—¥æœ¬èªã€100æ–‡å­—ä»¥å†…ã€‚æŠ•è³‡åˆ¤æ–­ã«å½¹ç«‹ã¤å…·ä½“çš„ãªäº‹å®Ÿã‚’ä¸­å¿ƒã«ï¼‰"
    }}
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯å°æ¹¾æ ªã®å°‚é–€ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚å³æ ¼ã«æƒ…å ±ã®ä¾¡å€¤ã‚’åˆ¤å®šã—ã¾ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.0
        )
        
        result = json.loads(response.choices[0].message.content)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã‹ã¤ã€Œé–¢é€£ãªã—ã€ã®å ´åˆã€é…å»¶ä¾¡å€¤åˆ¤å®šã‚’è©¦è¡Œ
        if is_fallback_mode and not result['is_relevant']:
            is_valuable, reason = is_delayed_but_valuable(entry, stock_info)
            if is_valuable:
                result['is_relevant'] = True
                result['reason'] = f"ã€é…å»¶ä¾¡å€¤ã‚ã‚Šã€‘{reason}"
                
        return result
        
    except Exception as e:
        return {"is_relevant": False, "reason": f"ã‚¨ãƒ©ãƒ¼: {e}", "summary": ""}

def process_stock_news(stock_code, stock_info, all_entries, is_fallback_mode=False):
    """ç‰¹å®šã®éŠ˜æŸ„ã«é–¢ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å‡¦ç†"""
    print(f"============================================================", flush=True)
    print(f"ğŸ“Š {stock_info['name']}ï¼ˆ{stock_code}ï¼‰", flush=True)
    print(f"============================================================", flush=True)
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ1æ¬¡é¸åˆ¥ï¼‰
    keywords = [stock_info['name'], stock_code]
    if 'keywords' in stock_info:
        keywords.extend(stock_info['keywords'])
        
    candidates = []
    for entry in all_entries:
        text = (entry['title'] + ' ' + entry['summary']).lower()
        if any(k.lower() in text for k in keywords):
            candidates.append(entry)
            
    print(f"å€™è£œãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(candidates)}ä»¶", flush=True)
    
    # å€™è£œãŒå¤šã™ãã‚‹å ´åˆã¯çµã‚Šè¾¼ã¿ï¼ˆLLMã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ä¸Šé™ã‚’å³ã—ãã™ã‚‹ï¼ˆ10ä»¶ï¼‰ã€é€šå¸¸æ™‚ã¯60ä»¶
    limit = 10 if is_fallback_mode else 60
    if len(candidates) > limit:
        candidates = sorted(candidates, key=lambda x: x['published'] or datetime.min, reverse=True)[:limit]
    
    # 2. LLMã«ã‚ˆã‚‹é–¢é€£æ€§åˆ¤å®š
    relevant_news = []
    news_summaries_for_aux = [] # æŠ•è³‡åˆ¤æ–­è£œåŠ©ç”¨ã®è¦ç´„ãƒªã‚¹ãƒˆ
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_entry = {
            executor.submit(analyze_relevance_with_llm, entry, stock_code, stock_info, is_fallback_mode): entry 
            for entry in candidates
        }
        
        for future in as_completed(future_to_entry):
            entry = future_to_entry[future]
            try:
                result = future.result()
                if result['is_relevant']:
                    entry['llm_result'] = result
                    relevant_news.append(entry)
                    news_summaries_for_aux.append(result['summary'])
                    print(f"  âœ… {entry['title'][:30]}...", flush=True)
                else:
                    # print(f"  âŒ {entry['title'][:30]}...", flush=True)
                    pass
            except Exception:
                pass
                
    print(f"âœ… é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(relevant_news)}ä»¶", flush=True)
    
    # 3. ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆv5.1æ©Ÿèƒ½ï¼‰
    clustered_news = []
    if relevant_news:
        # å¼•æ•°é †åºä¿®æ­£: (stock_name, relevant_news)
        clusters = cluster_news_by_topic(stock_info['name'], relevant_news)
        print_clustering_log(stock_info['name'], clusters)
        clustered_news = prepare_delivery_news(clusters)
        print(f"âœ… é…ä¿¡: {len(clustered_news)}ã‚¯ãƒ©ã‚¹ã‚¿", flush=True)
    
    # 4. æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”Ÿæˆï¼ˆv5.3æ–°æ©Ÿèƒ½ï¼‰
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªãã¦ã‚‚æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚‹ã®ã§å¿…ãšç”Ÿæˆã™ã‚‹
    print(f"ğŸ“‰ æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...", flush=True)
    
    # æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—
    price_data = get_stock_price_data(stock_code)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    investment_aux = generate_investment_aux_news(
        stock_code, 
        stock_info['name'], 
        price_data, 
        news_summaries_for_aux
    )
    print(f"âœ… æŠ•è³‡åˆ¤æ–­è£œåŠ©ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†", flush=True)
    
    return {
        'stock_code': stock_code,
        'stock_name': stock_info['name'],
        'news': clustered_news,
        'investment_aux': investment_aux, # è¿½åŠ 
        'news_count': len(relevant_news)
    }

def main():
    print("============================================================", flush=True)
    print(f"å°æ¹¾æ ªãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ  {VERSION}", flush=True)
    print("============================================================", flush=True)
    
    # ç¬¬1æ®µéš: ç›´è¿‘7æ—¥ãƒ¢ãƒ¼ãƒ‰
    print("=== ç¬¬1æ®µéš: ç›´è¿‘7æ—¥ãƒ¢ãƒ¼ãƒ‰ ===", flush=True)
    all_entries = fetch_rss_feeds(days_back=7)
    all_entries = resolve_redirects(all_entries)
    
    results = {}
    stocks_needing_fallback = []
    
    for stock_code, stock_info in STOCKS.items():
        result = process_stock_news(stock_code, stock_info, all_entries, is_fallback_mode=False)
        results[stock_code] = result
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒ0ä»¶ã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾è±¡ã«è¿½åŠ 
        if result['news_count'] == 0:
            stocks_needing_fallback.append(stock_code)
            
    # ç¬¬2æ®µéš: 30æ—¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¯¾è±¡éŠ˜æŸ„ã®ã¿ï¼‰
    if stocks_needing_fallback:
        print("\n=== ç¬¬2æ®µéš: 30æ—¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ ===", flush=True)
        print(f"å¯¾è±¡éŠ˜æŸ„: {', '.join(stocks_needing_fallback)}", flush=True)
        
        # éå»30æ—¥åˆ†ã®RSSã‚’å–å¾—ï¼ˆã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ã€å¯¾è±¡éŠ˜æŸ„ã®ã‚¯ã‚¨ãƒªã«çµã‚‹ã®ãŒç†æƒ³ã ãŒã€ä»Šå›ã¯ç°¡æ˜“çš„ã«å…¨å–å¾—ï¼‰
        # â€»æœ¬æ¥ã¯ã“ã“ã§ã‚¯ã‚¨ãƒªã‚’çµã‚‹ã¹ãã ãŒã€RSS_FEEDSã®æ§‹é€ ä¸Šã€å…¨å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹
        fallback_entries = fetch_rss_feeds(days_back=30)
        fallback_entries = resolve_redirects(fallback_entries)
        
        for stock_code in stocks_needing_fallback:
            stock_info = STOCKS[stock_code]
            print(f"ğŸ”„ {stock_info['name']} ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’é–‹å§‹...", flush=True)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å†å‡¦ç†
            result = process_stock_news(stock_code, stock_info, fallback_entries, is_fallback_mode=True)
            
            # çµæœã‚’ä¸Šæ›¸ãï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿ã€ã‚ã‚‹ã„ã¯0ä»¶ã§ã‚‚æŠ•è³‡åˆ¤æ–­è£œåŠ©ã¯ã‚ã‚‹ã®ã§æ›´æ–°ï¼‰
            results[stock_code] = result

    # ãƒ¡ãƒ¼ãƒ«ä½œæˆã¨é€ä¿¡
    print("\nğŸ“§ ãƒ¡ãƒ¼ãƒ«ä½œæˆä¸­...", flush=True)
    email_body = create_email_body(results)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    with open("email_preview.html", "w", encoding="utf-8") as f:
        f.write(email_body)
    print("  ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¿å­˜: email_preview.html", flush=True)
    
    print("ğŸš€ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ä¸­...", flush=True)
    status_code = send_email_via_sendgrid(
        api_key=SENDGRID_API_KEY,
        from_email=RECIPIENT_EMAIL,
        to_email=RECIPIENT_EMAIL,
        subject=f"ã€å°æ¹¾æ ªã€‘æœ¬æ—¥ã®æŠ•è³‡åˆ¤æ–­ãƒ¬ãƒãƒ¼ãƒˆ ({datetime.now(JST).strftime('%Y/%m/%d')})",
        html_content=email_body
    )
    
    if 200 <= status_code < 300:
        print("âœ… é€ä¿¡æˆåŠŸï¼", flush=True)
    else:
        print(f"âŒ é€ä¿¡å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {status_code}", flush=True)

if __name__ == "__main__":
    main()
